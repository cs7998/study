      Click on the ^^^ Quiz Portal tab above to open your quiz console

                     ALL SET!

controlplane $
controlplane $
controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE     VERSION
controlplane   Ready    master   2m31s   v1.18.0
node01         Ready    <none>   112s    v1.18.0
controlplane $ kubectl version --shortClient Version: v1.18.0
Server Version: v1.18.0
controlplane $ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
blue-8455cd8cd7-6qxj7   1/1     Running   0          103s   10.244.1.5   node01   <none>           <none>
blue-8455cd8cd7-8qcn4   1/1     Running   0          103s   10.244.1.7   node01   <none>           <none>
blue-8455cd8cd7-bbg2f   1/1     Running   0          103s   10.244.1.6   node01   <none>           <none>
blue-8455cd8cd7-rbkwf   1/1     Running   0          103s   10.244.1.8   node01   <none>           <none>
blue-8455cd8cd7-xpqbn   1/1     Running   0          103s   10.244.1.9   node01   <none>           <none>
red-59d898f784-725cm    1/1     Running   0          104s   10.244.1.4   node01   <none>           <none>
red-59d898f784-fs8mt    1/1     Running   0          104s   10.244.1.3   node01   <none>           <none>
controlplane $ kubectl describe node controlplane
Name:               controlplane
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=controlplane
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        flannel.alpha.coreos.com/backend-data: null
                    flannel.alpha.coreos.com/backend-type: host-gw
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 172.17.0.85
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 15 Jan 2021 16:16:24 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  controlplane
  AcquireTime:     <unset>
  RenewTime:       Fri, 15 Jan 2021 16:21:39 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------  -----------------                 ------------------                ------              -------
  NetworkUnavailable   False   Fri, 15 Jan 2021 16:17:10 +0000   Fri, 15 Jan 2021 16:17:10 +0000   FlannelIsUp              Flannel is running on this node
  MemoryPressure       False   Fri, 15 Jan 2021 16:19:12 +0000   Fri, 15 Jan 2021 16:16:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 15 Jan 2021 16:19:12 +0000   Fri, 15 Jan 2021 16:16:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 15 Jan 2021 16:19:12 +0000   Fri, 15 Jan 2021 16:16:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 15 Jan 2021 16:19:12 +0000   Fri, 15 Jan 2021 16:17:20 +0000   KubeletReady              kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.17.0.85
  Hostname:    controlplane
Capacity:
  cpu:                2
  ephemeral-storage:  199545168Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2040680Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  183900826525
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1938280Ki
  pods:               110
System Info:
  Machine ID:                 200df2a9c61f27fbb8eaec415fb272e4
  System UUID:                200df2a9c61f27fbb8eaec415fb272e4
  Boot ID:                    ba0250f2-4f57-48d7-97d4-84337b0c2a6c
  Kernel Version:             4.15.0-122-generic
  OS Image:                   Ubuntu 18.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://19.3.13
  Kubelet Version:            v1.18.0
  Kube-Proxy Version:         v1.18.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-66bff467f8-6dd4b                100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     4m53s
  kube-system                 coredns-66bff467f8-vthvs                100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     4m53s
  kube-system                 etcd-controlplane                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m
  kube-system                 kube-apiserver-controlplane             250m (12%)    0 (0%)      0 (0%)           0 (0%)         5m
  kube-system                 kube-controller-manager-controlplane    200m (10%)    0 (0%)      0 (0%)           0 (0%)         5m
  kube-system                 kube-flannel-ds-amd64-5wt4b             100m (5%)     100m (5%)   50Mi (2%)        50Mi (2%)      4m53s
  kube-system                 kube-keepalived-vip-kwtrf               0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m6s
  kube-system                 kube-proxy-j5jpc                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m53s
  kube-system                 kube-scheduler-controlplane             100m (5%)     0 (0%)      0 (0%)           0 (0%)         5m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%)   100m (5%)
  memory             190Mi (10%)  390Mi (20%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                    From                      Message
  ----    ------                   ----                   ----                      -------
  Normal  NodeHasSufficientMemory  5m37s (x8 over 5m38s)  kubelet, controlplane     Node controlplane status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m37s (x7 over 5m38s)  kubelet, controlplane     Node controlplane status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m37s (x8 over 5m38s)  kubelet, controlplane     Node controlplane status is now: NodeHasSufficientPID
  Normal  Starting                 5m1s                   kubelet, controlplane     Starting kubelet.
  Normal  NodeHasSufficientMemory  5m1s                   kubelet, controlplane     Node controlplane status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m1s                   kubelet, controlplane     Node controlplane status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m1s                   kubelet, controlplane     Node controlplane status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m                     kubelet, controlplane     Updated Node Allocatable limit across pods
  Normal  Starting                 4m45s                  kube-proxy, controlplane  Starting kube-proxy.
  Normal  NodeReady                4m19s                  kubelet, controlplane     Node controlplane status is now: NodeReady
controlplane $ kubectl describe node controlplane | grep -i taint
Taints:             <none>
controlplane $ kubectl describe node node01 | grep -i taint
Taints:             <none>
controlplane $ kubectl get deployments.apps
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   5/5     5            5           4m12s
red    2/2     2            2           4m13s
controlplane $ kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
blue-8455cd8cd7-6qxj7   1/1     Running   0          4m51s
blue-8455cd8cd7-8qcn4   1/1     Running   0          4m51s
blue-8455cd8cd7-bbg2f   1/1     Running   0          4m51s
blue-8455cd8cd7-rbkwf   1/1     Running   0          4m51s
blue-8455cd8cd7-xpqbn   1/1     Running   0          4m51s
red-59d898f784-725cm    1/1     Running   0          4m52s
red-59d898f784-fs8mt    1/1     Running   0          4m52s
controlplane $ kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
blue-8455cd8cd7-6qxj7   1/1     Running   0          5m     10.244.1.5   node01   <none>           <none>
blue-8455cd8cd7-8qcn4   1/1     Running   0          5m     10.244.1.7   node01   <none>           <none>
blue-8455cd8cd7-bbg2f   1/1     Running   0          5m     10.244.1.6   node01   <none>           <none>
blue-8455cd8cd7-rbkwf   1/1     Running   0          5m     10.244.1.8   node01   <none>           <none>
blue-8455cd8cd7-xpqbn   1/1     Running   0          5m     10.244.1.9   node01   <none>           <none>
red-59d898f784-725cm    1/1     Running   0          5m1s   10.244.1.4   node01   <none>           <none>
red-59d898f784-fs8mt    1/1     Running   0          5m1s   10.244.1.3   node01   <none>           <none>
controlplane $ kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.0
[upgrade/versions] kubeadm version: v1.18.0
I0115 16:24:56.477094   22999 version.go:252] remote version is much newer: v1.20.2; falling back to: stable-1.18
[upgrade/versions] Latest stable version: v1.18.15
[upgrade/versions] Latest stable version: v1.18.15
[upgrade/versions] Latest version in the v1.18 series: v1.18.15
[upgrade/versions] Latest version in the v1.18 series: v1.18.15

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     2 x v1.18.0   v1.18.15

Upgrade to the latest version in the v1.18 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.18.0   v1.18.15
Controller Manager   v1.18.0   v1.18.15
Scheduler            v1.18.0   v1.18.15
Kube Proxy           v1.18.0   v1.18.15
CoreDNS              1.6.7     1.6.7
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.18.15

Note: Before you can perform this upgrade, you have to update kubeadm to v1.18.15.

_____________________________________________________________________

controlplane $ kubectl drain controlplane
node/controlplane cordoned
error: unable to drain node "controlplane", aborting command...

There are pending nodes to be drained:
 controlplane
error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-amd64-5wt4b, kube-system/kube-keepalived-vip-kwtrf, kube-system/kube-proxy-j5jpc
controlplane $ kubectl drain controlplane --ignore-daemonsets
node/controlplane already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-5wt4b, kube-system/kube-keepalived-vip-kwtrf, kube-system/kube-proxy-j5jpc
evicting pod kube-system/coredns-66bff467f8-6dd4b
evicting pod kube-system/coredns-66bff467f8-vthvs
pod/coredns-66bff467f8-vthvs evicted
pod/coredns-66bff467f8-6dd4b evicted
node/controlplane evicted
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE     VERSION
controlplane   Ready,SchedulingDisabled   master   10m     v1.18.0
node01         Ready                      <none>   9m50s   v1.18.0
controlplane $ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.0", GitCommit:"9e991415386e4cf155a24b1da15becaa390438d8", GitTreeState:"clean", BuildDate:"2020-03-25T14:56:30Z", GoVersion:"go1.13.8", Compiler:"gc", Platform:"linux/amd64"}
controlplane $ apt-get install -y kubeadm=1.19.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
  libc-ares2 libhttp-parser2.7.1 libnetplan0 libuv1 nodejs-doc python3-netifaces
Use 'apt autoremove' to remove them.
The following additional packages will be installed:
  kubernetes-cni
The following packages will be upgraded:
  kubeadm kubernetes-cni
2 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.
Need to get 32.8 MB of archives.
After this operation, 21.3 MB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.8.7-00 [25.0 MB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.19.0-00 [7,759 kB]
Fetched 32.8 MB in 2s (13.9 MB/s)
(Reading database ... 145433 files and directories currently installed.)
Preparing to unpack .../kubernetes-cni_0.8.7-00_amd64.deb ...
Unpacking kubernetes-cni (0.8.7-00) over (0.7.5-00) ...
Preparing to unpack .../kubeadm_1.19.0-00_amd64.deb ...
Unpacking kubeadm (1.19.0-00) over (1.18.0-00) ...
Setting up kubernetes-cni (0.8.7-00) ...
Setting up kubeadm (1.19.0-00) ...
controlplane $ kubeadm verion
unknown command "verion" for "kubeadm"

Did you mean this?
        version

To see the stack trace of this error execute with --v=5 or higher
controlplane $ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.0", GitCommit:"e19964183377d0ec2052d1f1fa930c4d7575bd50", GitTreeState:"clean", BuildDate:"2020-08-26T14:28:32Z", GoVersion:"go1.15", Compiler:"gc", Platform:"linux/amd64"}
controlplane $ kubeadm upgrade apply v1.19.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.19.0"
[upgrade/versions] Cluster version: v1.18.0
[upgrade/versions] kubeadm version: v1.19.0
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.19.0"...
Static pod: kube-apiserver-controlplane hash: 283b3e27188ddea56dfca5e440da8c88
Static pod: kube-controller-manager-controlplane hash: f9b9c6969be80756638e9cf4927b5881
Static pod: kube-scheduler-controlplane hash: 5795d0c442cb997ff93c49feeb9f6386
[upgrade/etcd] Upgrading to TLS for etcd
Static pod: etcd-controlplane hash: be7db3aa5a0631d2b398899a5289635f
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-15-16-31-12/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: etcd-controlplane hash: be7db3aa5a0631d2b398899a5289635f
Static pod: etcd-controlplane hash: be7db3aa5a0631d2b398899a5289635f
Static pod: etcd-controlplane hash: 2477c928e79422cb47a8271d0ebe8240
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests080819565"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-15-16-31-12/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-controlplane hash: 283b3e27188ddea56dfca5e440da8c88
Static pod: kube-apiserver-controlplane hash: 283b3e27188ddea56dfca5e440da8c88
Static pod: kube-apiserver-controlplane hash: 7a3da820837c7b79aeda28907eb65b28
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed upold manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-15-16-31-12/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-controlplane hash: f9b9c6969be80756638e9cf4927b5881
Static pod: kube-controller-manager-controlplane hash: 819e512564af90a9bac2be65e4c615de
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-15-16-31-12/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-controlplane hash: 5795d0c442cb997ff93c49feeb9f6386
Static pod: kube-scheduler-controlplane hash: 23d2ea3ba1efa3e09e8932161a572387
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.19" in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.19.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
controlplane $ kubectl version --short
Client Version: v1.18.0
Server Version: v1.19.0
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
controlplane   Ready,SchedulingDisabled   master   18m   v1.18.0
node01         Ready                      <none>   17m   v1.18.0
controlplane $ apt install kubelet=1.19.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
  libc-ares2 libhttp-parser2.7.1 libnetplan0 libuv1 nodejs-doc python3-netifaces
Use 'apt autoremove' to remove them.
The following packages will be upgraded:
  kubelet
1 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.
Need to get 18.2 MB of archives.
After this operation, 3,281 kB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.19.0-00 [18.2 MB]
Fetched 18.2 MB in 1s (15.7 MB/s)
(Reading database ... 145436 files and directories currently installed.)
Preparing to unpack .../kubelet_1.19.0-00_amd64.deb ...
Unpacking kubelet (1.19.0-00) over (1.18.0-00) ...
Setting up kubelet (1.19.0-00) ...
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
controlplane   Ready,SchedulingDisabled   master   20m   v1.19.0
node01         Ready                      <none>   19m   v1.18.0
controlplane $ kubectl uncordon controlplane
node/controlplane uncordoned
controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   22m   v1.19.0
node01         Ready    <none>   21m   v1.18.0
controlplane $ kubectl drain node01
node/node01 cordoned
error: unable to drain node "node01", aborting command...

There are pending nodes to be drained:
 node01
error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-amd64-w4tjn, kube-system/kube-keepalived-vip-r59qh, kube-system/kube-proxy-fbqh9
controlplane $ kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-w4tjn, kube-system/kube-keepalived-vip-r59qh, kube-system/kube-proxy-fbqh9
evicting pod default/blue-8455cd8cd7-8qcn4
evicting pod kube-system/katacoda-cloud-provider-c57c46f4d-jck65
evicting pod default/blue-8455cd8cd7-6qxj7
evicting pod default/blue-8455cd8cd7-bbg2f
evicting pod default/blue-8455cd8cd7-rbkwf
evicting pod default/blue-8455cd8cd7-xpqbn
evicting pod default/red-59d898f784-725cm
evicting pod default/red-59d898f784-fs8mt
evicting pod kube-system/coredns-f9fd979d6-hxzfs
evicting pod kube-system/coredns-f9fd979d6-jrqqd
pod/blue-8455cd8cd7-6qxj7 evicted
I0115 16:39:53.149168   15022 request.go:621] Throttling request took 1.064547371s, request: GET:https://172.17.0.85:6443/api/v1/namespaces/default/pods/blue-8455cd8cd7-xpqbn
pod/blue-8455cd8cd7-rbkwf evicted
pod/blue-8455cd8cd7-xpqbn evicted
pod/red-59d898f784-fs8mt evicted
pod/katacoda-cloud-provider-c57c46f4d-jck65 evicted
pod/blue-8455cd8cd7-bbg2f evicted
pod/red-59d898f784-725cm evicted
pod/blue-8455cd8cd7-8qcn4 evicted
pod/coredns-f9fd979d6-hxzfs evicted
pod/coredns-f9fd979d6-jrqqd evicted
node/node01 evicted
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
controlplane   Ready                      master   23m   v1.19.0
node01         Ready,SchedulingDisabled   <none>   23m   v1.18.0
controlplane $ ssh node01
Warning: Permanently added 'node01,172.17.0.86' (ECDSA) to the list of known hosts.
node01 $ apt install kubeadm=1.19.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
  libc-ares2 libhttp-parser2.7.1 libnetplan0 libuv1 nodejs-doc python3-netifaces
Use 'apt autoremove' to remove them.
The following additional packages will be installed:
  kubernetes-cni
The following packages will be upgraded:
  kubeadm kubernetes-cni
2 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.
Need to get 32.8 MB of archives.
After this operation, 21.3 MB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.8.7-00 [25.0 MB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.19.0-00 [7,759 kB]
Fetched 32.8 MB in 2s (16.3 MB/s)
(Reading database ... 145440 files and directories currently installed.)
Preparing to unpack .../kubernetes-cni_0.8.7-00_amd64.deb ...
Unpacking kubernetes-cni (0.8.7-00) over (0.7.5-00) ...
Preparing to unpack .../kubeadm_1.19.0-00_amd64.deb ...
Unpacking kubeadm (1.19.0-00) over (1.18.0-00) ...
Setting up kubernetes-cni (0.8.7-00) ...
Setting up kubeadm (1.19.0-00) ...
node01 $ kubeadm upgrade node
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks
[preflight] Skipping prepull. Not a control plane node.
[upgrade] Skipping phase. Not a control plane node.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.
node01 $ apt install -y kubelet=1.19.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages were automatically installed and are no longer required:
  libc-ares2 libhttp-parser2.7.1 libnetplan0 libuv1 nodejs-doc python3-netifaces
Use 'apt autoremove' to remove them.
The following packages will be upgraded:
  kubelet
1 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.
Need to get 18.2 MB of archives.
After this operation, 3,281 kB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.19.0-00 [18.2 MB]
Fetched 18.2 MB in 1s (23.7 MB/s)
(Reading database ... 145443 files and directories currently installed.)
Preparing to unpack .../kubelet_1.19.0-00_amd64.deb ...
Unpacking kubelet (1.19.0-00) over (1.18.0-00) ...
Setting up kubelet (1.19.0-00) ...
node01 $ logout
Connection to node01 closed.
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
controlplane   Ready                      master   28m   v1.19.0
node01         Ready,SchedulingDisabled   <none>   27m   v1.19.0
controlplane $ kubectl uncordon node01
node/node01 uncordoned
controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   29m   v1.19.0
node01         Ready    <none>   28m   v1.19.0
controlplane $ Connection to host01 closed by remote host.
Connection to host01 closed.

The environment has expired.

Please refresh to get a new environment.