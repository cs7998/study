controlplane $
controlplane $
controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE    VERSION
controlplane   Ready    master   106s   v1.19.0                                                                   taknode01         Ready    <none>   69s    v1.19.0
node02         Ready    <none>   76s    v1.19.0
node03         Ready    <none>   70s    v1.19.0
controlplane $ kubectl get deployments.apps
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           19s
red    2/2     2            2           19s
controlplane $ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
blue-746c87566d-7ngfl   1/1     Running   0          60s   10.244.3.3   node01   <none>           <none>
blue-746c87566d-jqnq7   1/1     Running   0          60s   10.244.1.3   node02   <none>           <none>
blue-746c87566d-ns48j   1/1     Running   0          60s   10.244.2.3   node03   <none>           <none>
red-75f847bf79-s8t47    1/1     Running   0          60s   10.244.3.2   node01   <none>           <none>
red-75f847bf79-vd2mg    1/1     Running   0          60s   10.244.2.2   node03   <none>           <none>
controlplane $ kubectl drain node01
node/node01 cordoned
error: unable to drain node "node01", aborting command...

There are pending nodes to be drained:
 node01
error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-amd64-cq279, kube-system/kube-proxy-twdgl
controlplane $ kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-cq279, kube-system/kube-proxy-twdgl
evicting pod default/blue-746c87566d-7ngfl
evicting pod default/red-75f847bf79-s8t47
pod/blue-746c87566d-7ngfl evicted
pod/red-75f847bf79-s8t47 evicted
node/node01 evicted
controlplane $ kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
blue-746c87566d-dplz2   1/1     Running   0          50s    10.244.1.5   node02   <none>           <none>
blue-746c87566d-jqnq7   1/1     Running   0          4m3s   10.244.1.3   node02   <none>           <none>
blue-746c87566d-ns48j   1/1     Running   0          4m3s   10.244.2.3   node03   <none>           <none>
red-75f847bf79-vd2mg    1/1     Running   0          4m3s   10.244.2.2   node03   <none>           <none>
red-75f847bf79-wxs2d    1/1     Running   0          49s    10.244.1.4   node02   <none>           <none>
controlplane $ kubectl uncordon node01
node/node01 uncordoned
controlplane $ kubectl get node
NAME           STATUS   ROLES    AGE     VERSION
controlplane   Ready    master   6m44s   v1.19.0
node01         Ready    <none>   6m7s    v1.19.0
node02         Ready    <none>   6m14s   v1.19.0
node03         Ready    <none>   6m8s    v1.19.0
controlplane $ kubectl pod -o wide
Error: unknown command "pod" for "kubectl"

Did you mean this?
        top

Run 'kubectl --help' for usage.
controlplane $ kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
blue-746c87566d-dplz2   1/1     Running   0          2m7s    10.244.1.5   node02   <none>           <none>
blue-746c87566d-jqnq7   1/1     Running   0          5m20s   10.244.1.3   node02   <none>           <none>
blue-746c87566d-ns48j   1/1     Running   0          5m20s   10.244.2.3   node03   <none>           <none>
red-75f847bf79-vd2mg    1/1     Running   0          5m20s   10.244.2.2   node03   <none>           <none>
red-75f847bf79-wxs2d    1/1     Running   0          2m6s    10.244.1.4   node02   <none>           <none>
controlplane $ kubectl describe nodes master | grep -i taint
Error from server (NotFound): nodes "master" not found
controlplane $ kubectl describe nodes master
Error from server (NotFound): nodes "master" not found
controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE     VERSION
controlplane   Ready    master   9m3s    v1.19.0
node01         Ready    <none>   8m26s   v1.19.0
node02         Ready    <none>   8m33s   v1.19.0
node03         Ready    <none>   8m27s   v1.19.0
controlplane $ kubectl describe nodes controlplane | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule
controlplane $ kubectl drain node02 --ignore-daemonsets
node/node02 cordoned
error: unable to drain node "node02", aborting command...

There are pending nodes to be drained:
 node02
error: cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): default/hr-app
controlplane $ kubectl drain node02 --ignore-daemonsets --force
node/node02 already cordoned
WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/hr-app; ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-94swt, kube-system/kube-proxy-pttxr
evicting pod default/blue-746c87566d-jqnq7
evicting pod default/blue-746c87566d-dplz2
evicting pod default/hr-app
evicting pod default/red-75f847bf79-wxs2d
evicting pod kube-system/coredns-f9fd979d6-8kh5w
pod/hr-app evicted
pod/blue-746c87566d-dplz2 evicted
pod/blue-746c87566d-jqnq7 evicted
pod/red-75f847bf79-wxs2d evicted
pod/coredns-f9fd979d6-8kh5w evicted
node/node02 evicted
controlplane $ kubectl cordon node03
node/node03 cordoned
controlplane $ kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
controlplane   Ready                      master   14m   v1.19.0
node01         Ready                      <none>   14m   v1.19.0
node02         Ready,SchedulingDisabled   <none>   14m   v1.19.0
node03         Ready,SchedulingDisabled   <none>   14m   v1.19.0
controlplane $ Connection to host01 closed by remote host.
Connection to host01 closed.

The environment has expired.

Please refresh to get a new environment.